{"note":"Don't delete this file! It's used internally to help with page regeneration.","tagline":"Python library for manipulating CSV docs as columns","google":"","name":"csvcols","body":"This library takes a column-oriented approach towards CSV data. Everything is\r\nstored internally as Unicode, and everything is outwardly immutable. It has \r\nsupport for:\r\n\r\n* Parsing CSV files, including some Excel exported quirks\r\n* Selecting and renaming columns\r\n* Transforming documents by column\r\n* Re-sorting a document by columns or rows\r\n* Creating new documents by appending old ones together\r\n* Merging rows\r\n\r\nCSV files are everywhere and every language has a library to read them row by\r\nrow. But sometimes that's not the best way to look at it. You often want to \r\nmake manipulations, transform or make rule checks on certain columns. If you\r\nkeep the row by row model, then you just end up trying to jam everything into a\r\nsingle pass over the data. Or maybe you suck up everything into a 2D data\r\nstructure and edit it in several passes. But then you start having side-effects,\r\nand you're not sure what changed what. Then you want to add a new rule that\r\nrequires data from an older pass through the data, and you start making\r\ntemporary data structures to hold the values of special columns or rows. I've\r\nhad the 800 lb gorilla version of this thrown on my lap. It's a maintenance\r\nnightmare, and my frustrations with the code base inspired the creation of this\r\nlibrary.\r\n\r\nThe library in a nutshell:\r\n\r\n```python\r\nimport csvcols\r\nfrom csvcols import Column, S # S = shorthand for Selector\r\n\r\n# Read Document from file. If encoding is not specified, UTF-8 is assumed.\r\nraw_shipping_doc = csvcols.load(\"shipping_orders.csv\", encoding='latin-1')\r\n\r\n# Select a subset of the columns and make them into a new Document. While\r\n# we're doing this, we can rename or transform Columns.\r\nusers_doc = raw_shipping_doc.select(\r\n    S(\"email\", transform=unicode.lower),\r\n    S(\"BILLING_LAST\", rename=\"last_name\", transform=unicode.title),\r\n    S(\"BILLING_FIRST\", rename=\"first_name\"),\r\n    (\"CUSTOM 1\", \"special_notes\"), # We can use tuples for renames as well\r\n    \"country\" # Or simple strings if we don't want to do any transforms\r\n)\r\n\r\n# If the email, last name, and first initial match, merge the records \r\n# together, and keep the longer first name. By default, this sorts as well.\r\nmerged_doc = users_doc.merge_rows_on(\r\n    lambda row: (row.email, row.last_name, row.first_name[0]),\r\n    lambda r1, r2: r1 if len(r1.first_name) > len(r2.first_name) else r2\r\n)\r\n\r\n# Create a new Column based on existing data.\r\nis_edu_user_col = Column(\"Y\" if s.endswith(\".edu\") else \"N\"\r\n                         for s in merged_doc.email)\r\n\r\n# Append this new column to the doc (note: this creates a new doc)\r\nfinal_doc = merged_doc + (\"is_edu_user\", is_edu_user_col)\r\n\r\nprint cvscols.dumps(final_doc)\r\n```\r\n\r\nRecommendations\r\n---------------\r\nFor non-trivial work, try to break up your manipulations into stages, with each\r\nstage represented as a Document. It makes it much easier to isolate where things\r\nwent wrong and why. Also, you can use select() to break documents into logical \r\npieces. For instance, an orders invoice file might be broken up into \"users\",\r\n\"contact_info\", \"items\". It's much easier to follow if you have methods that \r\ntake a sub-document or just a few columns and operate on those, rather than \r\nhaving every method take a massive document and spit one back. You can later \r\nreconstruct the document by appending your pieces together. Just remember to \r\nrebuild the document with all the columns you care about before sorting or \r\nmerging.\r\n\r\nAlso, while csvcols will parse files into Columns of unicode data, it doesn't \r\nmean that you have to use unicode strings for all your Columns. If making an \r\nintermediate column datetime makes your life easier, by all means do it. The \r\nsame goes for having a real None value rather than overloading blanks to \r\nsometimes be an empty string and sometimes be a logical null. Remember that you\r\ncan serialize Columns and Documents as JSON, so you can store more complicated\r\ndata structures.\r\n\r\nIt's often the case that you have to flatten things out at the end to present it\r\nback as a CSV to the user or to a legacy system. But while that data is in\r\ntransit between Excel and some legacy horror, you have a richer vocabulary and\r\nshould use it. For instance, an error column might hold dictionaries that\r\nspecify severity, type, etc. Just please, please, for the sake of your sanity, \r\ndon't start mutating the rich data structures inside the Column if you go this\r\nroute. There's nothing I can do to stop you, but down that path lies madness.\r\n\r\nWarnings\r\n--------\r\nNo attempt has been made to make this library memory efficient or particularly \r\nfast. I didn't need it at this point, but it should be pretty feasible, since\r\nColumn data tends to be highly redundant in real life. I wrote a previous \r\nincarnation of this library that actually had a lot of transform hashing and\r\ncaching (the idea was to prevent full recalcuation of a series of transforms\r\nwhen only small parts of the document change), but it added more complexity\r\nthan it was worth, given how seldom I had a need for it.\r\n"}